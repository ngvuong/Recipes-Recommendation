{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "requests_cache.install_cache(\"cache5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dietary concerns(special-consideration): \n",
    "    * Quick and easy\n",
    "    * Healthy\n",
    "    * Organic\n",
    "    * Vegan\n",
    "    * Vegetarian\n",
    "* Ingredients (ingredient):\n",
    "    * Beef\n",
    "    * Chicken\n",
    "    * Fish\n",
    "    * Seafood\n",
    "    * Vegetable\n",
    "* Cuisine:\n",
    "    * African\n",
    "    * American\n",
    "    * Asian\n",
    "    * Central/ S. American\n",
    "    * European\n",
    "    * British\n",
    "    * Caribbean\n",
    "    * Indian\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up categories to scrape\n",
    "\n",
    "base_url = 'https://www.epicurious.com'\n",
    "\n",
    "categories = {'special-consideration': ['quick-and-easy', 'healthy', 'organic', 'vegan', 'vegetarian'], \n",
    "                'ingredient': ['beef', 'chicken', 'fish', 'seafood', 'vegetable'],\n",
    "               'cuisine': ['african', 'american', 'asian', 'central-south-american', 'european', \n",
    "                           'british', 'caribbean', 'indian']\n",
    "             }\n",
    "\n",
    "more_categories = {'meal':['side', 'dinner','lunch','breakfast'], \n",
    "                   'type':['salad','sandwich'], 'ingredient':['egg','rice','mushroom'],\n",
    "                  'technique':['barbecue','no-cook','fry','saute']}\n",
    "\n",
    "\n",
    "#Categories into list of dictionary for ease of scraping\n",
    "param_filter = list()\n",
    "for k,v in more_categories.items():\n",
    "    for e in v:\n",
    "        param_filter.append(dict([(k,e)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_url(param_dict, page_start = 1, page_max = 30, storage = rec_urls):\n",
    "    \n",
    "    '''\n",
    "    Requests and finds the url for for each recipe listed on a page (18 results per page)\n",
    "    param_dict: dictionary with key, val as param name:filter term, for Epicurious.com\n",
    "    page_start and page_max: starting and ending page to scrape for each category\n",
    "    storage: dictionary, holds title:url \n",
    "    '''\n",
    "    page = page_start\n",
    "    \n",
    "    while True:\n",
    "        url = f'{base_url}/search/'\n",
    "        base_params = {'content':'recipe', 'sort':'highestRated', 'page':page}\n",
    "        base_params.update(param_dict)\n",
    "        \n",
    "        req = requests.get(url, params=base_params)\n",
    "        if req.status_code == 404:    #in case page does not exist\n",
    "            break\n",
    "        \n",
    "        soup = bs(req.text, 'lxml')\n",
    "        \n",
    "        for tag in soup.find_all('a', class_='view-complete-item', itemprop='url'):\n",
    "            if tag['title'] not in storage and tag['title'] not in list(recipes.title):\n",
    "                storage[tag['title']] = f\"{base_url}{tag['href']}\"\n",
    "                print(tag['title'], base_url + tag['href'], param_dict, req.from_cache)\n",
    "                \n",
    "        page += 1\n",
    "        if page >= page_max:\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scraping urls and pickling\n",
    "'''\n",
    "\n",
    "# rec_urls = {}\n",
    "# for d in param_filter:\n",
    "#     get_url(d, page_start=30, page_max=100, storage=more_rec_urls)\n",
    "\n",
    "# with open('even_more_recs.p', 'wb') as file:\n",
    "#     pickle.dump(more_rec_urls, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# for d in param_filter:\n",
    "#     get_url(d, page_start=30, page_max=80)\n",
    "\n",
    "# with open('some_more_recs.p', 'wb') as file:\n",
    "#     pickle.dump(rec_urls, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('some_more_recs.p', 'rb') as file:\n",
    "#     dt = pickle.load(file)\n",
    "\n",
    "#Converting into DataFrame then to list of dictionary for ease of looping\n",
    "df_temp = pd.DataFrame(list(dt.items()), columns=['title', 'link'])\n",
    "rec_to_scrape = df_temp.to_dict('records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING out scraping each attribute of a recipe\n",
    "\n",
    "# url='https://www.epicurious.com/recipes/food/views/chicken-soup-with-caramelized-ginger'\n",
    "# req=requests.get(url)\n",
    "# soup = bs(req.text, 'lxml')\n",
    "\n",
    "\n",
    "# nutri = soup.find('div', class_='nutrition content')\n",
    "# nutri\n",
    "\n",
    "# categories = [e.text for e in soup.find_all('dt', itemprop='recipeCategory')]\n",
    "# date = soup.find('meta', itemprop='datePublished')['content']\n",
    "# desc = soup.find('div', itemprop='description').p.text\n",
    "# directions = [step.text.strip() for step in \n",
    "#               soup.find('div', class_='instructions').find_all('li', class_='preparation-step')]\n",
    "# ingr = [i.text for i in\n",
    "#        soup.find('div', class_='ingredients-info').find_all('li',class_='ingredient')]\n",
    "# rating = float(soup.find('span', class_='rating').text.split('/')[0]) * 5 / 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(rec_dict):\n",
    "    \n",
    "    url = rec_dict['link']\n",
    "    \n",
    "    req = requests.get(url)\n",
    "    soup = bs(req.text, 'lxml')\n",
    "    \n",
    "    nutri = soup.find('div', class_='nutrition content')\n",
    "    \n",
    "    try: \n",
    "        calories = float(nutri.find('span', itemprop='calories').text)\n",
    "    except AttributeError:\n",
    "        return None\n",
    "    try:\n",
    "        fat = float(nutri.find('span', itemprop='fatContent').text.split()[0])\n",
    "    except AttributeError:\n",
    "        fat = None\n",
    "    try:\n",
    "        protein = float(nutri.find('span', itemprop='proteinContent').text.split()[0])\n",
    "    except AttributeError:\n",
    "        protein = None\n",
    "    try:\n",
    "        carb = float(nutri.find('span', itemprop='carbohydrateContent').text.split()[0])\n",
    "    except AttributeError:\n",
    "        carb = None\n",
    "    try:\n",
    "        sodium = float(nutri.find('span', itemprop='sodiumContent').text.split()[0])\n",
    "    except AttributeError:\n",
    "        sodium = None\n",
    "    try:\n",
    "        categories = [cat.text for cat in soup.find_all('dt', itemprop='recipeCategory')]\n",
    "    except AttributeError:\n",
    "        categories = None\n",
    "    try:\n",
    "        date = soup.find('meta', itemprop='datePublished')['content']\n",
    "    except AttributeError:\n",
    "        date = None\n",
    "    try:\n",
    "        desc = soup.find('div', itemprop='description').p.text\n",
    "    except AttributeError:\n",
    "        desc = None\n",
    "    try:\n",
    "        directions = [step.text.strip() for step in \n",
    "                  soup.find('div', class_='instructions').find_all('li', class_='preparation-step')]\n",
    "    except AttributeError:\n",
    "        directions = None\n",
    "    try:\n",
    "        ingredients = [i.text for i in\n",
    "                    soup.find('div', class_='ingredients-info').find_all('li',class_='ingredient')]\n",
    "    except AttributeError:\n",
    "        ingredients = None\n",
    "    try:\n",
    "        rating = float(soup.find('span', class_='rating').text.split('/')[0]) * 5 / 4 #Scale to out of 5 rating\n",
    "    except AttributeError:\n",
    "        rating = None\n",
    " \n",
    "        \n",
    "    return (calories, fat, protein, carb, sodium, categories, date, desc, directions, ingredients, rating)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Scraping recipe info, add new key,val pair for each attribute of a recipe by\n",
    "unpacking resulting tuple from get_info function.\n",
    "'''\n",
    "\n",
    "# for rec in rec_to_scrape:\n",
    "#     res = get_info(rec)\n",
    "#     print(rec_to_scrape.index(rec)) \n",
    "#     if res:\n",
    "#         cal, fat, pro, carb, so, cat, date, desc, di, ingr, rating = get_info(rec)\n",
    "\n",
    "#         rec['calories'] = cal\n",
    "#         rec['fat'] = fat\n",
    "#         rec['protein'] = pro \n",
    "#         rec['carb'] = carb\n",
    "#         rec['sodium'] = so\n",
    "#         rec['categories'] = cat\n",
    "#         rec['date'] = date\n",
    "#         rec['desc'] = desc\n",
    "#         rec['directions'] = di\n",
    "#         rec['ingredients'] = ingr\n",
    "#         rec['rating'] = rating\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop problematic infinite values and missing data\n",
    "# new_recs_final = [e for e in rec_to_scrape if len(e) == 13 and e['calories'] != float('inf')]\n",
    "# \n",
    "# df = pd.DataFrame(new_recs_final)\n",
    "# with open('new_recs_final_df.pkl', 'wb') as file:\n",
    "#     pickle.dump(df, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# df.to_json('new_recs_final.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_carb(term):\n",
    "    \n",
    "    page = 1\n",
    "    while True:\n",
    "        params = {'content':'recipe', 'sort':'relevance', 'page':page}\n",
    "        search_url = f\"{base_url}/search/{term.replace(' ', '%20')}\"\n",
    "\n",
    "        req = requests.get(search_url, params=params)\n",
    "        if req.status_code == 404:\n",
    "            return (None, None)\n",
    "        soup = bs(req.text, 'lxml')\n",
    "\n",
    "        tag = soup.find('a', string=re.compile(term, re.I))\n",
    "        \n",
    "        if tag:\n",
    "            link = tag['href']\n",
    "            req2 = requests.get(f'{base_url}{link}')\n",
    "            soup2 = bs(req2.text, 'lxml')\n",
    "            date = soup2.find('meta' , itemprop='datePublished')['content']\n",
    "            try:\n",
    "                carb = float(soup2.find('span', itemprop='carbohydrateContent').text.split()[0])\n",
    "            except AttributeError:\n",
    "                carb = None\n",
    "            return (date, carb)\n",
    "        else:\n",
    "            page += 1\n",
    "        if page == 3:\n",
    "            print(f'Unsuccessful:{term}', req.from_cache, f'page{page}')\n",
    "            return (None, None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_recs = pd.read_json('clean_recs.json', convert_dates=False)\n",
    "# cr = clean_recs.to_dict('records')\n",
    "\n",
    "'''Chunking the recipes list into 3 parts,\n",
    "    ran simultaneously in 3 different notebooks\n",
    "'''\n",
    "# carb1 = copy.deepcopy(cr[:6000])\n",
    "# carb2 = copy.deepcopy(cr[6000:10000])\n",
    "# carb3 = copy.deepcopy(cr[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_carb(rec_list):\n",
    "    for d in rec_list:\n",
    "        rec = d['title'].replace(\"'\", \"\")\n",
    "        date, carb = get_carb(rec)\n",
    "        if date == d['date']:\n",
    "            d['carb'] = carb\n",
    "            print(f'{rec}, {carb}, {carb1.index(d)}')\n",
    "\n",
    "\n",
    "# loop_carb(carb1)\n",
    "# loop_carb(carb2)\n",
    "# loop_carb(carb3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('carb1.p', 'rb') as file:\n",
    "    carb1 = pickle.load(file)\n",
    "\n",
    "with open('carb2.p', 'rb') as file:\n",
    "    carb2 = pickle.load(file)\n",
    "\n",
    "with open('carb3.p', 'rb') as file:\n",
    "    carb3 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "carb = carb1 + carb2 + carb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(carb).to_json('old_recs_final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
